{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c4cbb0-138b-4d37-bdc5-6f2c61495ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151b8ccc629b403a82e5ab2309ec1a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4101' max='4101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4101/4101 4:05:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.206200</td>\n",
       "      <td>0.165459</td>\n",
       "      <td>0.970174</td>\n",
       "      <td>0.958476</td>\n",
       "      <td>0.947060</td>\n",
       "      <td>0.970174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155400</td>\n",
       "      <td>0.141575</td>\n",
       "      <td>0.973010</td>\n",
       "      <td>0.961596</td>\n",
       "      <td>0.950733</td>\n",
       "      <td>0.973010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.104371</td>\n",
       "      <td>0.976304</td>\n",
       "      <td>0.968832</td>\n",
       "      <td>0.964910</td>\n",
       "      <td>0.976304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1367' max='1367' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1367/1367 17:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10437119752168655, 'eval_accuracy': 0.9763037511436413, 'eval_f1': 0.9688320702458673, 'eval_precision': 0.9649103790616276, 'eval_recall': 0.9763037511436413, 'eval_runtime': 1078.9202, 'eval_samples_per_second': 10.13, 'eval_steps_per_second': 1.267, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rating\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Apply the rating prediction to each processed review\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed_Review\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(predict_rating)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Map predicted ratings to numeric values\u001b[39;00m\n\u001b[0;32m    103\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Rating_Num\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 96\u001b[0m, in \u001b[0;36mpredict_rating\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_rating\u001b[39m(text):\n\u001b[0;32m     95\u001b[0m     scores \u001b[38;5;241m=\u001b[39m rating_pipeline(text)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 96\u001b[0m     rating \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(scores)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Convert back to 1-indexed rating\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rating\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'scrapped_dhl.csv'  # Update this with the path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "df['Processed_Review'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "# Map rating scores to numeric values (1 to 5)\n",
    "df['label'] = df['Rating'] - 1  # Convert to 0-indexed for model training\n",
    "\n",
    "# Prepare dataset for training\n",
    "dataset = Dataset.from_pandas(df[['Processed_Review', 'label']])\n",
    "dataset = dataset.rename_column(\"Processed_Review\", \"text\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)  # 5 labels for ratings 1 to 5\n",
    "\n",
    "# Tokenize the dataset with padding and truncation\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Define the compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "# Create a prediction pipeline\n",
    "rating_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, return_all_scores=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f5f4e6-a8d1-4881-babc-59f812c0037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10221     0     0     0     0]\n",
      " [  103    15     0    11    17]\n",
      " [   17     8     0     9    16]\n",
      " [   15     7     0    12    31]\n",
      " [   21     3     0     1   423]]\n",
      "Overall Accuracy: 97.63%\n",
      "File saved as RoBERTa_Rating_Prediction_Results.csv\n"
     ]
    }
   ],
   "source": [
    "# Define a custom rating prediction function\n",
    "def predict_rating(text):\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    scores = outputs.logits.detach().cpu().numpy()[0]\n",
    "    rating = scores.argmax() + 1  # Convert back to 1-indexed rating\n",
    "    return rating\n",
    "\n",
    "# Apply the rating prediction to each processed review\n",
    "df['Predicted_Rating'] = df['Processed_Review'].apply(predict_rating)\n",
    "\n",
    "# Map predicted ratings to numeric values\n",
    "df['Predicted_Rating_Num'] = df['Predicted_Rating']\n",
    "\n",
    "# Calculate and print the confusion matrix and accuracy\n",
    "cm = confusion_matrix(df['Rating'], df['Predicted_Rating_Num'])\n",
    "accuracy = accuracy_score(df['Rating'], df['Predicted_Rating_Num'])\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_csv_path = 'RoBERTa_Rating_Prediction_Results.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"File saved as {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f85515f-418d-447b-bcd5-3524ad431e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'scrapped_dhl.csv'  # Update this with the path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "df['Processed_Review'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df['Processed_Review']\n",
    "y = df['Rating']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3a41fc-9487-465a-8c9a-8ec08566f3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.944647758462946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      1.00      0.97      2031\n",
      "           2       0.00      0.00      0.00        41\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.92      0.42      0.58        81\n",
      "\n",
      "    accuracy                           0.94      2186\n",
      "   macro avg       0.37      0.28      0.31      2186\n",
      "weighted avg       0.91      0.94      0.92      2186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9423604757548033\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      1.00      0.97      2031\n",
      "           2       0.00      0.00      0.00        41\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.88      0.37      0.52        81\n",
      "\n",
      "    accuracy                           0.94      2186\n",
      "   macro avg       0.37      0.27      0.30      2186\n",
      "weighted avg       0.91      0.94      0.92      2186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.938700823421775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      1.00      0.97      2031\n",
      "           2       0.00      0.00      0.00        41\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.76      0.32      0.45        81\n",
      "\n",
      "    accuracy                           0.94      2186\n",
      "   macro avg       0.34      0.26      0.28      2186\n",
      "weighted avg       0.90      0.94      0.92      2186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test_tfidf)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "# Support Vector Machine\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm.predict(X_test_tfidf)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = rf.predict(X_test_tfidf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ec06c5-c9e7-461a-acb0-d6a7d367cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression Accuracy: 0.9492223238792314\n",
      "Best Logistic Regression Params: {'C': 100, 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      1.00      0.98      2031\n",
      "           2       0.00      0.00      0.00        41\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.76      0.64      0.70        81\n",
      "\n",
      "    accuracy                           0.95      2186\n",
      "   macro avg       0.34      0.33      0.33      2186\n",
      "weighted avg       0.92      0.95      0.93      2186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM Accuracy: 0.9483074107959744\n",
      "Best SVM Params: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.99      0.98      2031\n",
      "           2       0.00      0.00      0.00        41\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.76      0.65      0.70        81\n",
      "\n",
      "    accuracy                           0.95      2186\n",
      "   macro avg       0.34      0.33      0.34      2186\n",
      "weighted avg       0.92      0.95      0.93      2186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Accuracy: 0.9382433668801464\n",
      "Best Random Forest Params: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      1.00      0.97      2031\n",
      "           2       0.00      0.00      0.00        41\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.79      0.32      0.46        81\n",
      "\n",
      "    accuracy                           0.94      2186\n",
      "   macro avg       0.35      0.26      0.28      2186\n",
      "weighted avg       0.90      0.94      0.92      2186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'scrapped_dhl.csv'  # Update this with the path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "df['Processed_Review'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df['Processed_Review']\n",
    "y = df['Rating']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Define hyperparameter grid for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# Define hyperparameter grid for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Define hyperparameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform Grid Search for Logistic Regression\n",
    "grid_search_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_search_lr.fit(X_train_tfidf, y_train)\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "y_pred_lr = best_lr.predict(X_test_tfidf)\n",
    "print(\"Best Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Best Logistic Regression Params:\", grid_search_lr.best_params_)\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Perform Grid Search for SVM\n",
    "grid_search_svm = GridSearchCV(SVC(), param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_search_svm.fit(X_train_tfidf, y_train)\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "y_pred_svm = best_svm.predict(X_test_tfidf)\n",
    "print(\"Best SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Best SVM Params:\", grid_search_svm.best_params_)\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Perform Grid Search for Random Forest\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train_tfidf, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test_tfidf)\n",
    "print(\"Best Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Best Random Forest Params:\", grid_search_rf.best_params_)\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "266e72e8-861d-4a14-8363-1bb96de1c038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'scrapped_dhl.csv'  # Update this with the path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "df['Processed_Review'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df['Processed_Review']\n",
    "y = df['Rating']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eea213f-b87c-480d-a261-a8e7f2135abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Accuracy: 0.929551692589204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96      2031\n",
      "           2       0.00      0.00      0.00        41\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       1.00      0.01      0.02        81\n",
      "\n",
      "    accuracy                           0.93      2186\n",
      "   macro avg       0.39      0.20      0.20      2186\n",
      "weighted avg       0.90      0.93      0.90      2186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Multinomial Naive Bayes model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_tfidf, y_train)\n",
    "y_pred_mnb = mnb.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the MNB model\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_mnb))\n",
    "print(classification_report(y_test, y_pred_mnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78f30bce-27fd-40cc-8e92-f16e80f44c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "try:\n",
    "    from numpy import triu\n",
    "except ImportError:\n",
    "    from scipy.linalg import triu\n",
    "\n",
    "# Load word2vec and GloVe embeddings\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "# Function to create embedding matrix\n",
    "def create_embedding_matrix(model, tokenizer, vocab_size, embedding_dim):\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i < vocab_size:\n",
    "            try:\n",
    "                embedding_vector = model[word]\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                continue\n",
    "    return embedding_matrix\n",
    "\n",
    "# Tokenize the data\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = 100  # Adjust as necessary\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Create embedding matrices\n",
    "embedding_dim = 300\n",
    "word2vec_embedding_matrix = create_embedding_matrix(word2vec_model, tokenizer, vocab_size, embedding_dim)\n",
    "glove_embedding_matrix = create_embedding_matrix(glove_model, tokenizer, vocab_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "496ec125-7aa0-4a1e-9195-2c4d0ca0d725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_4004\\2574192442.py\", line 18, in <module>\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 318, in fit\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 357, in _compute_loss\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 325, in compute_loss\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 609, in __call__\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 645, in call\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\losses\\loss.py\", line 43, in __call__\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\losses\\losses.py\", line 27, in call\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\losses\\losses.py\", line 1853, in sparse_categorical_crossentropy\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\ops\\nn.py\", line 1567, in sparse_categorical_crossentropy\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 645, in sparse_categorical_crossentropy\n\nReceived a label value of 5 which is outside the valid range of [0, 1).  Label values: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 2 1 1 1 1 1 2 1 4 1 1 1\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_8727]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train and evaluate CLSTM with Word2Vec\u001b[39;00m\n\u001b[0;32m     17\u001b[0m clstm_word2vec \u001b[38;5;241m=\u001b[39m build_clstm_model(word2vec_embedding_matrix, vocab_size, embedding_dim, max_length)\n\u001b[1;32m---> 18\u001b[0m clstm_word2vec\u001b[38;5;241m.\u001b[39mfit(X_train_padded, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)  \u001b[38;5;66;03m# Adjust epochs as necessary\u001b[39;00m\n\u001b[0;32m     19\u001b[0m y_pred_clstm_word2vec \u001b[38;5;241m=\u001b[39m clstm_word2vec\u001b[38;5;241m.\u001b[39mpredict(X_test_padded)\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLSTM with Word2Vec Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_test, y_pred_clstm_word2vec))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_4004\\2574192442.py\", line 18, in <module>\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 318, in fit\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 357, in _compute_loss\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 325, in compute_loss\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 609, in __call__\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 645, in call\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\losses\\loss.py\", line 43, in __call__\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\losses\\losses.py\", line 27, in call\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\losses\\losses.py\", line 1853, in sparse_categorical_crossentropy\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\ops\\nn.py\", line 1567, in sparse_categorical_crossentropy\n\n  File \"C:\\Users\\krish\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 645, in sparse_categorical_crossentropy\n\nReceived a label value of 5 which is outside the valid range of [0, 1).  Label values: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 2 1 1 1 1 1 2 1 4 1 1 1\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_8727]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "\n",
    "def build_clstm_model(embedding_matrix, vocab_size, embedding_dim, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='softmax'))  # Update the output layer to match your number of classes (5)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate CLSTM with Word2Vec\n",
    "clstm_word2vec = build_clstm_model(word2vec_embedding_matrix, vocab_size, embedding_dim, max_length)\n",
    "clstm_word2vec.fit(X_train_padded, y_train, epochs=5, verbose=1, validation_split=0.2)  # Adjust epochs as necessary\n",
    "y_pred_clstm_word2vec = clstm_word2vec.predict(X_test_padded).argmax(axis=1) + 1\n",
    "\n",
    "print(\"CLSTM with Word2Vec Accuracy:\", accuracy_score(y_test, y_pred_clstm_word2vec))\n",
    "print(classification_report(y_test, y_pred_clstm_word2vec))\n",
    "\n",
    "# Train and evaluate CLSTM with GloVe\n",
    "clstm_glove = build_clstm_model(glove_embedding_matrix, vocab_size, embedding_dim, max_length)\n",
    "clstm_glove.fit(X_train_padded, y_train, epochs=5, verbose=1, validation_split=0.2)  # Adjust epochs as necessary\n",
    "y_pred_clstm_glove = clstm_glove.predict(X_test_padded).argmax(axis=1) + 1\n",
    "\n",
    "print(\"CLSTM with GloVe Accuracy:\", accuracy_score(y_test, y_pred_clstm_glove))\n",
    "print(classification_report(y_test, y_pred_clstm_glove))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f79cdfe-66fd-4158-8aec-8a6f7013d8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
